# 网络爬虫文档

本文档详细说明了项目中每个爬虫的逻辑、数据采集方法。

## 爬虫详细说明

### 1. Token合约爬虫 (token_scraper.py)
- **目的**: 监控以太坊区块链上新建立的ERC20代币合约
- **逻辑流程**:
    - 每5秒检查最新区块
    - 识别区块中的合约创建交易
    - 验证是否为ERC20代币合约
- **采集数据**:
    - 合约地址
    - 代币名称和符号
    - 总供应量和小数位数
    - 创建区块号码和时间戳
    - 合约拥有者地址

### 2. 合约源码爬虫 (contracts_scraper.py)
- **目的**: 从Etherscan获取智能合约源码
- **逻辑流程**:
    - 每1分钟检查新代币
    - 每5分钟重试获取缺失源码
- **采集数据**:
    - 合约源码
    - 编译器版本
    - 优化设置
    - 许可证类型
    - 代理合约信息

### 3. Twitter数据爬虫 (twitter_scraper.py)
- **目的**: 收集代币相关的Twitter活动数据
- **逻辑流程**:
    - 每1分钟更新新Twitter用户
    - 每5分钟更新推文数据
- **采集数据**:
    - 用户资料和创建时间
    - 推文内容和时间戳
    - 互动数据(点赞、转发、评论)
    - 提及的账户列表

### 4. 社交媒体提取器 (social_media_extractor.py)
- **目的**: 从合约源码中提取社交媒体链接
- **逻辑流程**: 
    - 扫描合约源码
    - 使用正则表达式匹配URL
    - 验证并清理URL
- **采集数据**:
    - Twitter账号和用户名
    - Telegram群组链接
    - 项目网站URL

### 5. Dextool数据爬虫 (dextool_scraper.py)
- **目的**: 收集代币在DEX上的安全审计数据
- **逻辑流程**:
    - 检查代币合约地址
    - 获取审计结果和配对地址
- **采集数据**:
    - 合约安全评分
    - 蜜罐检测结果
    - 税率信息
    - 潜在风险警告
- **执行方式**: `python3 dextool_scraper.py`

## 执行说明
1. 单独运行爬虫或使用 `runScraperScirpt.sh` 进行批量执行

## 数据存储
- 所有数据存储在SQLite数据库中
- 自动记录错误日志
- 定期数据备份机制

## 错误处理
- 内建重试机制
- 详细的错误日志记录
- 异常情况自动通知